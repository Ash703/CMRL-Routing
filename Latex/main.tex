\documentclass[12pt, a4paper]{report}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage{graphicx} % For images
\usepackage{geometry} % Margins
\usepackage{titlesec} % Header formatting
\usepackage{hyperref} % Clickable links
\usepackage{amsmath}  % Math equations
\usepackage{booktabs} % Professional tables
\usepackage{float}    % Image positioning
\usepackage{listings} % For code snippets
\usepackage{xcolor}   % Colors for code
\usepackage[style=ieee, backend=biber]{biblatex}     % For better citation handling
\addbibresource{references.bib}
% --- PAGE GEOMETRY ---
\geometry{top=2.5cm, bottom=2.5cm, left=2.0cm, right=2.0cm}

% --- CODE STYLE ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% =================================================================
% 1. TITLE PAGE
% =================================================================
\begin{document}

\begin{titlepage}
    \centering
    \vspace*{1cm}
    
    \Huge
    \textbf{Dynamic Flow Scheduling in Leaf Spine Topology for Data Center Networks}
    
    \vspace{1.5cm}
    
    % \Large
    % \textit{A Project Report Submitted in partial fulfillment of the requirements for the course}
    
    \vspace{0.5cm}
    \textbf{Advanced Computer Networks (CS G525)}
    
    \vspace{2cm}
    
    \textbf{Submitted By:}
    
    \vspace{0.5cm}
    \large
    Ashish Shetty (ID: 2025H1030064P) \\
    S Rishab (ID: 2025H1030066P)
    
    \vspace{2cm}
    
    \begin{figure}[h]
        \centering
        \framebox{\parbox{3cm}{\centering \vspace{1cm} \vspace{1cm}}
        \includegraphics[width=0.3\textwidth]{BITSLogo.jpg}}
    \end{figure}
    
    % \vspace{2cm}
    
    \Large
    \textbf{Department of Computer Science and Information System} \\
    \textbf{Birla Institute of Technology and Science, Pilani} \\
    \today
    
\end{titlepage}

% =================================================================
% 2. ABSTRACT
% =================================================================
\begin{abstract}

Data centers increasingly rely on leaf–spine topologies to provide high bandwidth, fault tolerance, and scalability. However, traffic in such networks is highly dynamic, with mice flows (short, latency-sensitive) and elephant flows (long, bandwidth-intensive) often competing for resources.  Traditional routing mechanisms like Equal-Cost Multi-Path (ECMP) rely on static hashing, often routing multiple large flows onto the same link while others remain idle. 
\par
\vspace{0.5cm}
This project proposes a dynamic, intelligent routing framework leveraging Software-Defined Networking (SDN) and Deep Reinforcement Learning (DRL). The project implements an Actor-Critic RL agent within the Ryu controller to manage a Leaf-Spine topology emulated in Mininet. The system utilizes real-time network states, throughput, link utilization, utilization skew, and packet loss to dynamically reroute  flows. Experimental results demonstrate that the RL-based approach achieves higher average throughput and better load balancing compared to ECMP, Static, and Greedy routing algorithms.
\end{abstract}

% =================================================================
% 3. TABLE OF CONTENTS
% =================================================================
\tableofcontents
\listoffigures
\newpage

% =================================================================
% 4. INTRODUCTION
% =================================================================
\chapter{Introduction}

\section{Background and Motivation}
Modern Data Centers host diverse applications ranging from web services to distributed machine learning training. The traffic patterns in these networks are highly bimodal; the majority of flows are small latency-sensitive "mice," while a few large "elephant" flows consume the bulk of the bandwidth. Static routing protocols such as OSPF and standard load balancing techniques such as ECMP fails to account for the real-time utilization of links, leading to inefficient resource usage and congestion. \par
Regular routing techniques also do not take into account the context of flows or the condition of the links and switches. And in the case of dynamic single path routing, elephant flows with multiple parallel connections aren't taken into consideration. These limitations motivate the need for adaptive, context-aware routing that can respond to instantaneous network conditions. Reinforcement Learning (RL) provides a promising alternative because it can incorporate fine-grained link statistics, flow behavior, and historical performance into its decision process. By learning a policy that explicitly accounts for utilization, packet loss, and flow type, an RL-based controller can make proactive routing decisions, promoting large flows to weighted multi-path forwarding while letting small flows follow lightweight single-path rules. Such an approach has the potential to significantly reduce congestion, improve throughput fairness, and increase overall fabric efficiency in modern data-center networks.


\section{Problem Statement}
Data centers require flows that are both high bandwidth and low latency, no matter the type or context of the flow. It should be able to deliver Quality of Service to elephant flows, mice flows, interactive flows, etc. and should be able to dynamically handle link utilization between its switches. In a Leaf-Spine topology based Data Center Network the concept of using the shortest path fails as there are multiple equally shortest path between the hosts and equal cost multi-path doesn't take the link utilization into consideration. Hence a flow scheduling algorithm which would manage utilization based on the flow stats received from the switches is necessary.


\includegraphics[width=0.5\textwidth]{image.png}
\section{Objectives}
\begin{itemize}
    \item To design a Leaf-Spine topology using Mininet.
    \item To generate artificial traffic using the iPerf3 to simulate the environment.
    \item To implement an SDN Controller using Ryu that gathers real-time port statistics.
    \item To develop an Actor-Critic Reinforcement Learning model to make intelligent routing decisions.
    \item To ensure Quality of Service (QoS)ness and fair among the different competing flows within the network.
    \item To compare the RL performance against ECMP, Static, and Greedy algorithms.
\end{itemize}

% =================================================================
% 5. LITERATURE REVIEW
% =================================================================
\chapter{Literature Review}

\section{Traditional and Heuristic Routing Algorithms}

Traffic engineering in Data Center Networks (DCNs) has evolved from static configurations to dynamic, load-aware scheduling. This section reviews the fundamental algorithms currently deployed in production environments and identifies their limitations regarding "Elephant" flow management.

\subsection{Static and Shortest Path Routing}
The most fundamental approach to routing relies on static protocols or Shortest Path First (SPF) algorithms like OSPF (Open Shortest Path First). In these systems, the path between a source and destination is pre-determined based on hop count or link weights.\vspace{0.5cm}

\noindent\textbf{Mechanism:} A flow is assigned a path solely based on network topology, ignoring current traffic conditions.

\noindent\textbf{Limitation:} Static routing suffers severely from ``Head-of-Line Blockin''. If a high-bandwidth flow saturates a specific link, all subsequent flows assigned to that link experience congestion, even if alternative paths in the Leaf-Spine topology are idle.

\subsection{Equal-Cost Multi-Path (ECMP)}
To utilize the bisection bandwidth of Leaf-Spine topologies, the industry standard is Equal-Cost Multi-Path (ECMP) routing\cite{10.17487/RFC2992}.\vspace{0.5cm}

\noindent\textbf{Mechanism:} ECMP distributes traffic across available paths using a hash of the packet header's 5-tuple (Source IP, Destination IP, Source Port, Destination Port, Protocol). This ensures that packets belonging to the same flow always take the same path, preventing TCP packet reordering.

\noindent\textbf{Limitation:} While ECMP provides excellent load balancing for many small "Mice" flows, it is ``traffic-oblivious". It does not account for the size of the flow or the current utilization of the links. A phenomenon known as "Hash Collision" occurs when two or more ``Elephant" flows coincidentally hash to the same physical link while other links remain underutilized. This results in significant throughput degradation for high-priority applications.

\subsection{Heuristic and Greedy Approaches}
With the advent of Software-Defined Networking (SDN), centralized controllers allowed for global visibility and dynamic re-routing. Several heuristic algorithms were proposed to address the limitations of ECMP.\vspace{0.5cm}

\textbf{The Greedy Algorithm:}
The "Greedy" or "Least-Loaded" approach represents a logical improvement over static hashing. The greedy algorithm  makes locally optimal choices at each Hop, hoping to find a global optimum solution. 

\noindent\textbf{Mechanism:} The controller periodically polls switch statistics. When a new flow arrives (or is detected), the controller assigns it to the path with the lowest instantaneous utilization.

\noindent\textbf{Limitation:} While effective in theory, Greedy algorithms often suffer from ``Route Flapping" or oscillation. If Path A is empty, the algorithm shifts all new traffic to Path A, causing it to become congested immediately. In the next polling cycle, it shifts everything to Path B. This oscillation increases jitter and creates instability in TCP congestion control mechanisms.\vspace{0.5cm}

\textbf{Hedera\cite{10.5555/1855711.1855730} and Mahout\cite{5934956}:}
Seminal works such as Hedera and Mahout introduced the concept of distinguishing flows.

\noindent\textbf{Mechanism:} These systems utilize a central scheduler to detect flows exceeding a specific bandwidth threshold (Elephants). Once detected, these flows are explicitly scheduled onto non-conflicting paths using algorithms like Global First Fit, while Mice flows are left to ECMP.
\noindent\textbf{Limitation:} These approaches typically rely on complex optimization solvers that may not converge quickly enough for highly dynamic traffic patterns, or they rely on slow polling intervals (e.g., 5 seconds) which may miss bursty traffic spikes.


\section{Reinforcement Learning in Networking}
Conventional routing algorithms, such as ECMP and Greedy heuristics, operate on rigid, pre-defined rules that often fail to adapt to the non-linear and highly dynamic nature of modern Data Center Networks (DCNs). Consequently, the research community has shifted toward Knowledge-Defined Networking (KDN), employing Machine Learning to enable autonomous network management. Specifically, Reinforcement Learning (RL) has emerged as a powerful paradigm for solving online routing optimization problems by formulating them as Markov Decision Processes (MDPs).

\subsection{Real-Time DRL Optimization}
Recent literature emphasizes the capability of Deep Reinforcement Learning (DRL) to handle complex network states that hand-crafted heuristics cannot manage. As discussed in \textit{Towards Real-Time Routing Optimization with Deep Reinforcement Learning} \textbf{RT-RO\cite{9481864}}, DRL offers a distinct advantage in highly dynamic scenarios where traditional control systems fall short. 

\noindent\textbf{Mechanism:} The mechanism relies on an agent that continuously interacts with the network environment, learning optimal decision-making policies through trial and error without requiring a pre-constructed mathematical model of the traffic pattern. This allows for real-time operation, addressing the computational latency issues often found in traditional optimization solvers.

\subsection{DRL-R (Multi-Resource Optimization)}
In the context of Software-Defined Data-Center Networks (SDN-DCN), simple bandwidth metrics are often insufficient. Liu et al. introduced \textbf{DRL-R\cite{drl_r}}, a Deep Reinforcement Learning-based routing scheme that innovates by recombining multiple network resources, specifically bandwidth, cache, and computing power—into a unified state metric. 

\noindent\textbf{Mechanism:} Their mechanism quantifies the contribution of cache in reducing delay, allowing the DRL agent deployed on the SDN controller to make routing decisions based on a "resource-recombined" state. They implemented and compared Deep Q-Network (DQN) and Deep Deterministic Policy Gradient (DDPG) agents, observing that policy-gradient methods (like DDPG) generally outperformed value-based methods (DQN) in terms of throughput and flow completion time.

\subsection{Delay-Aware DRL Agent}
Further validating this approach, recent studies\cite{delay_aware} on Deep-RL for SDN routing optimization have demonstrated that agents can automatically adapt to current traffic conditions to propose tailored configurations. 

\noindent\textbf{Mechanism:} The primary mechanism here is the minimization of a specific reward function based on network delay. By allowing the agent to observe the global state, the system provides significant operational advantages over traditional optimization algorithms, which often struggle to converge quickly during traffic bursts.

% \subsection{CDPFS (Centralized Dynamic Parallel Flow Scheduling)}
% While not strictly RL, the move toward centralized, global-state decision-making is a critical precursor to SDN-based RL. In their work on dynamic parallel flow algorithms, researchers supplemented BCube topologies with a central master computer to execute algorithms like \textbf{CDPFS}. 
% \textbf{Mechanism:} The mechanism focuses on analyzing global network states to identify the least congested paths for specific flows and allocating them in parallel. This centralized view significantly reduces "flow collisions"—a major limitation of distributed routing—and improves bandwidth utilization, a concept central to the design of modern RL-based SDN controllers.

\subsection{Proposed System: Actor-Critic for Elephant Flow Routing}
Drawing from the insights above, this project implements a centralized control loop using the \textbf{Actor-Critic}\cite{6392457} Reinforcement Learning architecture. While frameworks like DRL-R utilize DDPG, and others explore DQN, this project leverages Actor-Critic to combine the stability of Policy Gradient methods with the sample efficiency of Value-based methods.

This project specifically targets the ``Elephant flow" problem. Unlike general traffic routing, here the agent utilizes a global view of the Leaf-Spine topology to detect high-bandwidth flows and dynamically re-route them based on a reward function that balances three competing objectives: maximizing throughput, minimizing link utilization skew (load balancing), and minimizing packet loss.

% =================================================================
% 6. METHODOLOGY
% =================================================================
\chapter{Methodology and System Design}

\section{System Architecture}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Spine-Leaf.jpg}
    \caption{Leaf–Spine Topology}
    \label{fig:arch}
\end{figure}

The system comprises three planes:
\begin{enumerate}
    \item \textbf{Data Plane:} Emulated via Mininet using Open vSwitch (OVS). It consists of a leaf-spine topology network.
    \item \textbf{Control Plane:} The Ryu SDN Controller. It communicates via OpenFlow 1.3 to install flow rules and query statistics.
    \item \textbf{Intelligence Plane:} A PyTorch-based Deep Learning model embedded within the controller.
\end{enumerate}

\section{The RL Formulation}

\subsection{State Space ($S_t$)}
The agent observes a state vector representing the condition of all available uplink paths from the leaf switch to the spine layer:
\[
S_t = [U_1, L_1, U_2, L_2, \dots, U_K, L_K]
\]
where $U_k$ denotes the normalized bandwidth utilization and $L_k$ denotes the normalized packet loss rate of the $k$-th uplink path. This representation captures both congestion and reliability indicators for every possible next-hop spine switch.

\subsection{Action Space ($A_t$)}
The action corresponds to choosing one of the $K$ uplinks (spine switches) for forwarding traffic:
\[
A_t \in \{0, 1, \dots, K-1\}.
\]

The action is consumed differently depending on the flow type:
\begin{itemize}
    \item \textbf{Mice Flows:} A single best path is selected using 
    \[
    a_t = \arg\max_a \pi(a \mid S_t),
    \]
    ensuring low overhead and fast forwarding.
    \item \textbf{Elephant Flows:} Instead of a single action index, the full probability vector 
    \[
    \pi(a \mid S_t) = [p_1, p_2, \dots, p_K]
    \]
    is converted into OpenFlow group weights, enabling weighted load balancing across multiple uplinks.
\end{itemize}

Thus, the same policy is reused for both flow categories, but its output is interpreted differently based on flow size.

\subsection{Reward Function ($R_t$)}
The reinforcement learning agent is optimized using a weighted reward function designed to jointly encourage high throughput, balanced load distribution, and low packet loss:
\[
R_t = \alpha \cdot \text{Throughput}_{\text{norm}}
\;-\;
\beta \cdot \text{Skew}_{\text{util}}
\;-\;
\gamma \cdot \text{Loss}_{\text{norm}}
\]
where:
\begin{itemize}
    \item $\text{Throughput}_{\text{norm}}$ is the normalized throughput of the flow,
    \item $\text{Skew}_{\text{util}}$ is the standard deviation of uplink utilizations (a measure of load imbalance),
    \item $\text{Loss}_{\text{norm}}$ is the normalized packet loss on the chosen path.
\end{itemize}

In this work, the reward weights are set to:
\[
\alpha = 2,\qquad \beta = 1,\qquad \gamma = 1,
\]
reflecting a stronger emphasis on maximizing throughput while still penalizing imbalance and packet loss.


\section{RL Model Design}
The reinforcement learning component in our system uses an Actor--Critic architecture to make routing decisions based on the real-time state of uplink ports on each leaf switch. The goal is to learn a policy that assigns flows to spine uplinks in a way that improves throughput while avoiding congestion.
\subsection{Neural Network Architecture}

Both the Actor and Critic are implemented using compact two-layer feed-forward neural networks.

\subsubsection*{Actor Network (Policy)}
\[
\text{Input dim} = 2*Numpaths(Spines),\qquad
\text{Hidden layers: } 64 \rightarrow 64,\qquad
\text{Output dim} = N
\]

The actor outputs logits that are converted to routing probabilities using a softmax function. The final probability vector:
\[
\pi(a \mid s)
\]
represents the routing distribution over uplinks.

\subsubsection*{Critic Network (Value Function)}
\[
\text{Input dim} = 2*Numpaths(Spines),\qquad
\text{Hidden layers: } 64 \rightarrow 64,\qquad
\text{Output dim} = 1
\]

The critic estimates the expected return for the current network state.

\subsection{Action Selection}

Given a state $s$, the actor computes:
\[
\pi(a \mid s) = \text{softmax}(f_\theta(s)),
\]
and an action is sampled from this distribution.  
Each action corresponds to choosing one spine uplink or multiple paths in case of elephant flows.

\subsection{Training Objective}

The critic is trained using a temporal-difference target:
\[
y = r + \gamma V(s'),
\]
and minimizes the squared error between $V(s)$ and $y$.

The actor uses an advantage-based policy-gradient update:
\[
A = y - V(s),
\]
which encourages actions that lead to higher throughput and balanced load.

An entropy regularization term is added to prevent premature convergence:
\[
\mathcal{L}_{entropy} = -\beta\, H(\pi).
\]

\section{Flow Promotion Technique}
In our RL-driven leaf–spine network controller, flow promotion is used to give large (“elephant”) flows dedicated load-balanced forwarding treatment. The controller continuously polls OpenFlow flow-stats from all leaf switches and computes the byte delta for each active IPv4 flow. When this instantaneous throughput exceeds a configurable threshold (2 MB per polling interval), the controller marks the flow as an elephant. Instead of allowing the flow to use the default single uplink chosen by the RL policy for mice flows, the controller installs a SELECT group table on the ingress leaf. This group contains weighted buckets, each corresponding to one of the spine uplinks. The RL policy outputs a probability distribution over available uplinks, and these probabilities are converted into bucket weights to influence traffic splitting. As a result, elephant flows receive more stable and load-balanced forwarding, while smaller mice flows continue using simple one-shot flows. This flow promotion mechanism ensures that large transfers are dynamically steered over the most favorable paths based on real-time port utilization and loss measurements.\par


The flow promotion mechanism tightly couples RL output with OpenFlow group programming. The RL agent continuously observes per-uplink network conditions through port-stats, normalized into a state vector comprising utilization and loss features. When an elephant flow is detected, the controller queries the RL actor to compute action probabilities. Instead of choosing a single output port (as is done for mice flows), the system interprets the full probability distribution as a multi-action decision. These probabilities are converted into integer weights that populate a SELECT group. Each bucket in the group corresponds to a potential egress uplink, and the hardware load balancer chooses paths proportional to the RL policy’s output. This allows the RL agent to express nuanced traffic-engineering strategies, distributing elephant flows across multiple spines while still biasing decisions toward paths that historically yield higher rewards. In this way, flow promotion becomes a direct actuation mechanism where the learned policy actively shapes the fabric’s load distribution.

\begin{figure} \centering \includegraphics[width=14cm]{FlowPromotion.png} \caption{High-level overview of System Plan} \label{fig:flowpromotion} \end{figure}
\pagebreak
\section{Tools and Technologies Used}
The implementation of the proposed Intelligent Routing System relies on a stack of open-source networking and machine learning tools.

\subsection{Network Emulation: Mininet}
Mininet\cite{Mininet} is used to emulate the Data Plane. It allows the creation of a realistic virtual network on a single kernel, supporting custom topologies and Open vSwitch (OVS) instances.  It executes the topology.py script to instantiate switches, links, and hosts, providing an isolated environment for testing routing logic without physical hardware. Thus, the required topology is obtained.

\subsection{SDN Controller: Ryu}
Ryu\cite{Ryu} is a component-based software-defined networking framework written in Python. It serves as the Control Plane.It acts as the centralized brain of the network. It listens for Packet-In events, gathers port statistics, and installs flow rules via the OpenFlow protocol. 

\subsection{Machine Learning Framework: PyTorch}
PyTorch\cite{PyTorch} is an open-source machine learning library used to implement the Intelligence Plane. It hosts the Deep Reinforcement Learning (DRL) agent. It constructs the Actor-Critic neural network, performs the forward pass for policy inference (routing decisions), and executes the backpropagation algorithm (training) using the calculated reward signals.

\subsection{Traffic Generation: iPerf3}
iPerf3\cite{iperf} is a standard tool for active measurements of the maximum achievable bandwidth on IP networks. It generates the synthetic workload to train and test the RL agent. It is scripted to generate dynamic flow patterns, including high-bandwidth TCP ``Elephant" flows, replicating real-world Data Center congestion scenarios.

\subsection{Python}
Python\cite{Python} is the prgramming language used for implementation. Python version 3.9 is last version compatible with the Ryu. Matplotlib\cite{Matplotlib}, which is Python package, is used for plotting the graphs.


%=================================================================
% 7. IMPLEMENTATION AND RESULTS
% =================================================================
\chapter{Implementation and Results}

\section{Experimental Setup}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{ourtopo.png}
    \caption{Primary Topology}
    \label{fig:topo}
\end{figure}

\begin{itemize}
    \item \textbf{Topology:} 3 Spines, 6 Leaves, 12 Hosts.
    \item \textbf{Traffic:} Iperf3 used to generate background UDP noise and a persistent TCP Elephant flow between Host 1 and Host 4.
    \item \textbf{Training:} The RL agent was trained for 1000 steps with a batch size of 8.
\end{itemize}

\section{Performance Metrics}
We evaluated the system based on:
\begin{enumerate}
    \item \textbf{Throughput:} Measures the effective data transfer rate achieved by the flow. Higher throughput indicates that the selected path provides adequate bandwidth and minimal congestion.
    
    \item \textbf{Packet Loss:} Inferred primarily from TCP retransmissions. Persistent loss suggests congestion, queue overflows, or suboptimal routing decisions.
    
    \item \textbf{Round-Trip Time (RTT):} Represents end-to-end latency. Lower RTT reflects faster acknowledgment cycles, while elevated RTT often correlates with queue buildup.
    
    \item \textbf{TCP Congestion Window (CWND):} Indicates how much data the sender can inject into the network without waiting for ACKs. A growing CWND implies stable conditions; stagnation or reduction indicates congestion.
    
    \item \textbf{Jitter:} Measures the variation in RTT over time. High jitter typically reflects unstable paths, fluctuating queue lengths, or inconsistent scheduling across links.
\end{enumerate}

\section{Result Discussion}


\subsection{Throughput Comparison}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_throughput_bar.png}
    \caption{Throughput Comparison}
    \label{fig:throughput-bar}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_throughput_timeseries.png}
    \caption{Throughput Timeseries of all methods}
    \label{fig:throughput-ts}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_throughput_timeseries_RL.png}
    \caption{Throughput Timeseries for proposed method}
    \label{fig:throughput-main}
\end{figure}
 The primary objective of this study was to evaluate the capacity of the Reinforcement Learning (RL) agent to maintain high data transfer rates for Elephant flows in the presence of network congestion. We compared the RL agent against five standard baselines: Equal-Cost Multi-Path (ECMP), Greedy , Static, Round Robin, and Random routing. The overall efficacy of each algorithm was measured by calculating the mean throughput of the monitored Elephant flow over the duration of the experiment. As illustrated in the figure, the RL agent was able to outperform other algorithms by a significant margin. 

\subsection{Latency Comparison}
% PLACEHOLDERS FOR YOUR GRAPHS
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_rtt_timeseries_rest.png}
    \caption{Latency Comparison of other methods}
    \label{fig:latency-others}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_rtt_timeseries.png}
    \caption{Latency of proposed model}
    \label{fig:latency}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_rtt_cdf_rest.png}
    \caption{Latency cdf of other methods}
    \label{fig:latency-cdf-others}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_rtt_cdf.png}
    \caption{Latency cdf of proposed method}
    \label{fig:latency-cdf}
\end{figure}
Even with such a high throughput, RTT of the connection managed by the RL agent was initially very low, but it increased due to exploration by the agent. But still it performed better than most algorithm. The Latency CDF graph shows that the proposed model is right in the middle.

\subsection{Packet Retransmition}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_retransmits.png}
    \caption{Retransmition Timeseries}
    \label{fig:rt-ts}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_retransmits_RL.png}
    \caption{Retransmition Timeseries for proposed method}
    \label{fig:rtrl-ts}
\end{figure}
Despite the high throughput, the proposed model has very low retransmission rate. As, the model learns environment better, the retransmission rate decreases. It remains constant except when model tries to explore better outcomes.
\subsection{TCP congestion window and Jitter}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_tcp_cwnd.png}
    \caption{TCP CWND for all methods}
    \label{fig:tcp-cwd}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_tcp_cwnd_RL.png}
    \caption{TCP CWND for proposed method}
    \label{fig:tcp-rl}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_tcp_jitter.png}
    \caption{TCP jitter for all methods}
    \label{fig:rtrl-ts}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graph_tcp_jitter_RL.png}
    \caption{TCP jitter for proposed method}
    \label{fig:rtrl-ts}
\end{figure}
The proposed model has very constant TCP congestion window suggesting that model penalizes packet drop, which has resulted in less packet drops. In terms of Jitter, the proposed model demonstrate slightly better than Greedy, Random and Static algorithm. 
\subsection{RL Agent Training}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{graph_rl_policy_boxplot.png}
    \caption{RL Agent Policy Boxplot}
    \label{fig:policy}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{graph_rl_loss_convergence.png}
    \caption{RL Agent Loss Convergence}
    \label{fig:lc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{graph_rl_probability_evolution.png}
    \caption{RL Agent Probability Evolution}
    \label{fig:pe}
\end{figure}
The Figure 4.15 shows the box plot of the probability distribution for the three spines. The second one is clearly preferred by the model. It is followed by the third spine, which also has the least range and outliers out of the IQR. The first spine is least preferred by the model.\par
The Figure 4.17 shows the similar trends with Spine1 losing the favourability towards the end. The third spine shows the least variation.\par
The figure 4.16 illustrates the agent's learning dynamics. The initial phase shows stable but slow learning. At around 55,000 seconds, a significant network event triggered a sharp increase in both Actor and Critic losses, forcing the agent out of a local optimum. The subsequent volatility indicates active learning, where the Critic constantly re-evaluates state values in response to the Actor's changing policy. The fact that the losses do not simply explode but oscillate within a bounded range suggests the agent is successfully updating its policy without diverging, actively adapting to the dynamic traffic conditions
% =================================================================
% 8. CONCLUSION
% =================================================================
\chapter{Conclusion}
This project demonstrates that Deep Reinforcement Learning (DRL) can serve as a practical and effective approach to dynamic traffic engineering in Software-Defined Networks in context of Data Center Network. By integrating an Actor-Critic agent directly into the Ryu controller, we enabled the control plane to make real-time routing decisions based on live network statistics such as port utilization and packet loss. The system successfully identifies high-bandwidth elephant flows, promotes them through selective group routing, and distributes them across multiple spine links based on predicted performance. Unlike traditional mechanisms such as ECMP, which assume uniform path quality and lack responsiveness to congestion, the DRL-based method adapts continuously to shifting network conditions.

Experimental results in our Mininet-based leaf-spine topology show that the RL agent learns to avoid hotspots, reduce flow collisions, and improve throughput across competing flows. By using measured rewards derived from throughput, utilization skew, and loss, the agent evolves policies that outperform static baselines and heuristic-driven routing. Moreover, the design supports real-time inference without imposing excessive overhead on the controller, demonstrating the feasibility of embedding learning-based logic into SDN control systems.

For future work, an idea could be extending the RL agent from per-leaf local decisions to a multi-agent or centralized global policy could unlock additional performance gains. Incorporating additional metrics such as latency, queue occupancy, or energy consumption may improve the agent’s situational awareness. On the deployment side, migrating from Ryu to P4-enabled programmable switches would allow running learned policies directly in the data plane, reducing decision latency and enabling line-rate execution. Finally, evaluating the system at larger scales and under more diverse traffic patterns would help validate its robustness for real data center workloads.

% =================================================================
% 9. REFERENCES
% =================================================================
\printbibliography[title={References}]

\end{document}